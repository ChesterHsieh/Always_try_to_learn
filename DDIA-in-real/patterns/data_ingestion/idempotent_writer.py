"""
Pattern Name: Idempotent Writer

Problem:
    If a data ingestion job fails and is retried, it may write duplicate records
    to the target dataset. This leads to data quality issues and incorrect
    aggregations downstream.

Solution:
    Use an idempotent writer that checks if records have already been written
    before inserting them. This is typically done using a unique identifier
    (e.g., record ID, hash, or composite key) to detect duplicates.

Use Cases:
    - Retrying failed batch ingestion jobs
    - Processing the same source data multiple times
    - Ensuring exactly-once semantics in distributed systems
    - Handling network retries and partial failures

Reference: Data Engineering Design Patterns (2025), Idempotent Writer pattern.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Callable, Iterable, Optional, Protocol, Sequence
import hashlib
import json
import logging

from patterns.utils.simple_db import SimpleDB


logger = logging.getLogger(__name__)


class IdempotentSink(Protocol):
    """Abstract target for idempotent writes.

    An IdempotentSink implementation is responsible for:
    - Checking if a record already exists (using a unique key)
    - Writing records only if they don't already exist
    - Optionally tracking write attempts for monitoring
    """

    def exists(self, record_id: str) -> bool:
        """Check if a record with the given ID already exists."""

    def write_if_not_exists(self, record_id: str, record: dict) -> bool:
        """Write a record only if it doesn't already exist.
        
        Returns:
            True if the record was written, False if it already existed.
        """


@dataclass
class SimpleDbIdempotentSink:
    """SimpleDB-backed sink that ensures idempotent writes.

    This sink uses record IDs to detect duplicates and only writes records
    that haven't been seen before.
    """

    db: SimpleDB
    table_name: str
    id_field: str = "id"  # Field name in the record that contains the unique ID

    def exists(self, record_id: str) -> bool:
        """Check if a record with the given ID already exists."""
        result = self.db.select(self.table_name, record_id)
        exists = result is not None
        if exists:
            logger.debug("Record with ID '%s' already exists in table '%s'", record_id, self.table_name)
        return exists

    def write_if_not_exists(self, record_id: str, record: dict) -> bool:
        """Write a record only if it doesn't already exist."""
        if self.exists(record_id):
            logger.info("Skipping duplicate record with ID '%s'", record_id)
            return False
        
        # Ensure table exists
        self.db.create_table(self.table_name)
        success = self.db.insert(self.table_name, record_id, record)
        if success:
            logger.info("Wrote new record with ID '%s' to table '%s'", record_id, self.table_name)
        return success


class IdempotentWriter:
    """High-level idempotent writer for data ingestion.

    This class ensures that records are only written once, even if the
    ingestion job is run multiple times.

    Docstring (ä¸­æ–‡èªªæ˜Ž):
        IdempotentWriter ç¢ºä¿å³ä½¿é‡è¤‡åŸ·è¡Œç›¸åŒçš„è³‡æ–™æ“·å–ä½œæ¥­ï¼Œä¹Ÿä¸æœƒç”¢ç”Ÿé‡è¤‡çš„è¨˜éŒ„ï¼š
        - ä½¿ç”¨å”¯ä¸€è­˜åˆ¥ç¢¼ï¼ˆIDï¼‰ä¾†æª¢æŸ¥è¨˜éŒ„æ˜¯å¦å·²å­˜åœ¨
        - åªå¯«å…¥å°šæœªå­˜åœ¨çš„è¨˜éŒ„
        - æ”¯æ´é‡è©¦æ©Ÿåˆ¶è€Œä¸æœƒç”¢ç”Ÿé‡è¤‡è³‡æ–™
    """

    def __init__(
        self,
        sink: IdempotentSink,
        extract_fn: Callable[[], Iterable[dict]],
        id_extractor: Optional[Callable[[dict], str]] = None,
        transform_fn: Optional[Callable[[Iterable[dict]], Iterable[dict]]] = None,
        on_duplicate: Optional[Callable[[str, dict], None]] = None,
    ) -> None:
        """
        Initialize an IdempotentWriter.

        Args:
            sink: Target sink implementing the idempotent write protocol.
            extract_fn: Function that returns an iterable of raw records.
            id_extractor: Optional function that extracts a unique ID from a record.
                If None, uses the 'id' field or generates a hash of the record.
            transform_fn: Optional function that transforms the iterable of records.
            on_duplicate: Optional callback invoked when a duplicate record is detected.
        """
        self._sink = sink
        self._extract_fn = extract_fn
        self._id_extractor = id_extractor or self._default_id_extractor
        self._transform_fn = transform_fn
        self._on_duplicate = on_duplicate

    def _default_id_extractor(self, record: dict) -> str:
        """Default ID extractor: use 'id' field or hash the record."""
        if "id" in record:
            return str(record["id"])
        # Generate a deterministic hash of the record
        record_str = json.dumps(record, sort_keys=True)
        return hashlib.md5(record_str.encode()).hexdigest()

    def run(self) -> tuple[int, int]:
        """Run the idempotent ingestion pipeline.

        Returns:
            A tuple of (records_written, duplicates_skipped).
        """
        # Extract
        raw_records = list(self._extract_fn())
        logger.info("IdempotentWriter: extracted %d records", len(raw_records))

        # Transform
        if self._transform_fn is not None:
            transformed_records = list(self._transform_fn(raw_records))
            logger.info(
                "IdempotentWriter: transformed %d records into %d records",
                len(raw_records),
                len(transformed_records),
            )
        else:
            transformed_records = raw_records

        # Write idempotently
        written = 0
        duplicates = 0

        for record in transformed_records:
            record_id = self._id_extractor(record)
            was_written = self._sink.write_if_not_exists(record_id, record)
            
            if was_written:
                written += 1
            else:
                duplicates += 1
                if self._on_duplicate is not None:
                    self._on_duplicate(record_id, record)

        logger.info(
            "IdempotentWriter: wrote %d new records, skipped %d duplicates",
            written,
            duplicates,
        )
        return written, duplicates


def demo_idempotent_writer() -> None:
    """Run a small demo of the Idempotent Writer pattern using SimpleDB.

    This demo simulates:
    - A list of events to be ingested
    - Running the ingestion twice to demonstrate idempotency
    - Showing that duplicates are skipped
    """
    print("=" * 60)
    print("ðŸ§ª Idempotent Writer Demo (SimpleDB)")
    print("=" * 60)

    db = SimpleDB()
    table_name = "events"
    sink = SimpleDbIdempotentSink(db=db, table_name=table_name, id_field="event_id")

    events = [
        {"event_id": "e1", "user_id": "u1", "action": "click", "timestamp": "2024-01-01T10:00:00Z"},
        {"event_id": "e2", "user_id": "u2", "action": "view", "timestamp": "2024-01-01T10:01:00Z"},
        {"event_id": "e3", "user_id": "u3", "action": "purchase", "timestamp": "2024-01-01T10:02:00Z"},
    ]

    def extract() -> Iterable[dict]:
        return list(events)

    def id_extractor(record: dict) -> str:
        return record["event_id"]

    writer = IdempotentWriter(
        sink=sink,
        extract_fn=extract,
        id_extractor=id_extractor,
    )

    # First run - should write all records
    print("\nðŸ“ First ingestion run:")
    written1, duplicates1 = writer.run()
    print(f"âœ… Wrote {written1} records, skipped {duplicates1} duplicates")
    db.show_data(table_name)

    # Second run - should skip all as duplicates
    print("\nðŸ“ Second ingestion run (idempotency test):")
    written2, duplicates2 = writer.run()
    print(f"âœ… Wrote {written2} records, skipped {duplicates2} duplicates")
    db.show_data(table_name)

    # Add a new event and run again
    events.append({"event_id": "e4", "user_id": "u4", "action": "logout", "timestamp": "2024-01-01T10:03:00Z"})
    print("\nðŸ“ Third ingestion run (with new event):")
    written3, duplicates3 = writer.run()
    print(f"âœ… Wrote {written3} records, skipped {duplicates3} duplicates")
    db.show_data(table_name)


# ============================================================================
# PySpark Implementation
# ============================================================================

try:
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col
    SPARK_AVAILABLE = True
except ImportError:
    SPARK_AVAILABLE = False


@dataclass
class SparkIdempotentSink:
    """Spark-based sink for idempotent writes.

    Uses Spark's DataFrame operations to check for existing records
    before writing, ensuring idempotency.
    """

    spark: SparkSession
    output_path: str
    id_column: str = "id"
    format: str = "delta"  # Delta Lake supports merge operations

    def exists(self, record_id: str) -> bool:
        """Check if a record with the given ID already exists."""
        try:
            existing_df = self.spark.read.format(self.format).load(self.output_path)
            count = existing_df.filter(col(self.id_column) == record_id).count()
            return count > 0
        except Exception:
            # Table doesn't exist yet
            return False

    def write_if_not_exists(self, record_id: str, record: dict) -> bool:
        """Write a record only if it doesn't already exist."""
        if self.exists(record_id):
            logger.info("Skipping duplicate record with ID '%s'", record_id)
            return False

        # Write new record
        df = self.spark.createDataFrame([record])
        writer = df.write.format(self.format).mode("append")
        writer.save(self.output_path)
        logger.info("Wrote new record with ID '%s'", record_id)
        return True


class SparkIdempotentWriter:
    """Spark-based idempotent writer for data ingestion."""

    def __init__(
        self,
        spark: SparkSession,
        sink: SparkIdempotentSink,
        extract_fn: Callable[[], Iterable[dict]],
        id_extractor: Optional[Callable[[dict], str]] = None,
        transform_fn: Optional[Callable[[Iterable[dict]], Iterable[dict]]] = None,
    ) -> None:
        if not SPARK_AVAILABLE:
            raise ImportError("PySpark is not available. Install with: pip install pyspark")

        self._spark = spark
        self._sink = sink
        self._extract_fn = extract_fn
        self._id_extractor = id_extractor or (lambda r: str(r.get("id", "")))
        self._transform_fn = transform_fn

    def run(self) -> tuple[int, int]:
        """Run the Spark idempotent ingestion pipeline."""
        raw_records = list(self._extract_fn())

        if self._transform_fn is not None:
            transformed_records = list(self._transform_fn(raw_records))
        else:
            transformed_records = raw_records

        written = 0
        duplicates = 0

        for record in transformed_records:
            record_id = self._id_extractor(record)
            if self._sink.write_if_not_exists(record_id, record):
                written += 1
            else:
                duplicates += 1

        return written, duplicates


# ============================================================================
# PyFlink Implementation
# ============================================================================

try:
    from pyflink.table import StreamTableEnvironment
    FLINK_AVAILABLE = True
except ImportError:
    FLINK_AVAILABLE = False


@dataclass
class FlinkIdempotentSink:
    """Flink-based sink for idempotent writes."""

    table_env: StreamTableEnvironment
    table_name: str
    id_column: str = "id"

    def exists(self, record_id: str) -> bool:
        """Check if a record with the given ID already exists."""
        try:
            result = self.table_env.execute_sql(
                f"SELECT COUNT(*) as cnt FROM {self.table_name} WHERE {self.id_column} = '{record_id}'"
            )
            # Simplified - real implementation would parse result
            return False
        except Exception:
            return False

    def write_if_not_exists(self, record_id: str, record: dict) -> bool:
        """Write a record only if it doesn't already exist."""
        if self.exists(record_id):
            return False

        # Insert record using Flink Table API
        # Simplified implementation
        logger.info("Wrote new record with ID '%s'", record_id)
        return True


class FlinkIdempotentWriter:
    """Flink-based idempotent writer for data ingestion."""

    def __init__(
        self,
        table_env: StreamTableEnvironment,
        sink: FlinkIdempotentSink,
        extract_fn: Callable[[], Iterable[dict]],
        id_extractor: Optional[Callable[[dict], str]] = None,
        transform_fn: Optional[Callable[[Iterable[dict]], Iterable[dict]]] = None,
    ) -> None:
        if not FLINK_AVAILABLE:
            raise ImportError("PyFlink is not available. Install with: pip install apache-flink")

        self._table_env = table_env
        self._sink = sink
        self._extract_fn = extract_fn
        self._id_extractor = id_extractor or (lambda r: str(r.get("id", "")))
        self._transform_fn = transform_fn

    def run(self) -> tuple[int, int]:
        """Run the Flink idempotent ingestion pipeline."""
        raw_records = list(self._extract_fn())

        if self._transform_fn is not None:
            transformed_records = list(self._transform_fn(raw_records))
        else:
            transformed_records = raw_records

        written = 0
        duplicates = 0

        for record in transformed_records:
            record_id = self._id_extractor(record)
            if self._sink.write_if_not_exists(record_id, record):
                written += 1
            else:
                duplicates += 1

        return written, duplicates

