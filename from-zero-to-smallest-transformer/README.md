# From Zero to Smallest Transformer

Building a transformer model from scratch in Rust without any third-party dependencies.

## é¡¹ç›®ç›®æ ‡ (Project Goals)

ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªå®Œæ•´çš„ Transformer æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š
- åŸºç¡€çš„æ•°å­¦è¿ç®—ï¼ˆçŸ©é˜µã€å‘é‡ã€æ¿€æ´»å‡½æ•°ï¼‰
- ç¥ç»ç½‘ç»œå±‚ï¼ˆçº¿æ€§å±‚ã€æ³¨æ„åŠ›æœºåˆ¶ã€å±‚å½’ä¸€åŒ–ï¼‰
- Transformer æ¶æ„ï¼ˆç¼–ç å™¨ã€è§£ç å™¨ï¼‰
- è®­ç»ƒå¾ªç¯ï¼ˆæŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ï¼‰

æ‰€æœ‰å®ç°ä»…ä½¿ç”¨ Rust æ ‡å‡†åº“ï¼Œä¸ä¾èµ–ä»»ä½•ç¬¬ä¸‰æ–¹åº“ã€‚

## é¡¹ç›®ç»“æ„ (Project Structure)

```
src/
â”œâ”€â”€ main.rs                 # ä¸»å…¥å£
â”œâ”€â”€ math/                   # æ•°å­¦è¿ç®—æ¨¡å—
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ matrix.rs          # çŸ©é˜µè¿ç®—
â”‚   â”œâ”€â”€ vector.rs          # å‘é‡è¿ç®—
â”‚   â””â”€â”€ activation.rs      # æ¿€æ´»å‡½æ•°ï¼ˆReLU, GELU, Softmaxç­‰ï¼‰
â”œâ”€â”€ nn/                     # ç¥ç»ç½‘ç»œå±‚
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ linear.rs          # çº¿æ€§å±‚ï¼ˆå…¨è¿æ¥å±‚ï¼‰
â”‚   â”œâ”€â”€ attention.rs       # å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
â”‚   â”œâ”€â”€ layer_norm.rs      # å±‚å½’ä¸€åŒ–
â”‚   â””â”€â”€ feed_forward.rs    # å‰é¦ˆç½‘ç»œ
â”œâ”€â”€ transformer/            # Transformer æ¶æ„
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ encoder.rs         # ç¼–ç å™¨å—
â”‚   â”œâ”€â”€ decoder.rs         # è§£ç å™¨å—
â”‚   â””â”€â”€ transformer.rs     # å®Œæ•´ Transformer æ¨¡å‹
â””â”€â”€ training/               # è®­ç»ƒç›¸å…³
    â”œâ”€â”€ mod.rs
    â”œâ”€â”€ loss.rs            # æŸå¤±å‡½æ•°ï¼ˆäº¤å‰ç†µã€MSEï¼‰
    â”œâ”€â”€ optimizer.rs       # ä¼˜åŒ–å™¨ï¼ˆSGDã€Adamï¼‰
    â””â”€â”€ trainer.rs         # è®­ç»ƒå™¨
```

## æ ¸å¿ƒç»„ä»¶ (Core Components)

### 1. æ•°å­¦è¿ç®— (Math Operations)
- **Matrix**: çŸ©é˜µè¿ç®—ï¼ˆä¹˜æ³•ã€åŠ æ³•ã€è½¬ç½®ç­‰ï¼‰
- **Vector**: å‘é‡è¿ç®—ï¼ˆç‚¹ç§¯ã€åŠ æ³•ç­‰ï¼‰
- **Activation Functions**: ReLU, GELU, Softmax, Sigmoid

### 2. ç¥ç»ç½‘ç»œå±‚ (Neural Network Layers)
- **Linear**: å…¨è¿æ¥å±‚
- **MultiHeadAttention**: å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
- **LayerNorm**: å±‚å½’ä¸€åŒ–
- **FeedForward**: å‰é¦ˆç½‘ç»œï¼ˆä¸¤å±‚çº¿æ€§å±‚ + GELUï¼‰

### 3. Transformer æ¶æ„
- **EncoderBlock**: Transformer ç¼–ç å™¨å—ï¼ˆè‡ªæ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ï¼‰
- **DecoderBlock**: Transformer è§£ç å™¨å—ï¼ˆè‡ªæ³¨æ„åŠ› + äº¤å‰æ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œï¼‰
- **Transformer**: å®Œæ•´çš„ Transformer æ¨¡å‹

### 4. è®­ç»ƒç»„ä»¶
- **CrossEntropyLoss**: äº¤å‰ç†µæŸå¤±å‡½æ•°
- **MSELoss**: å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°
- **SGD**: éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨
- **Adam**: Adam ä¼˜åŒ–å™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰
- **Trainer**: è®­ç»ƒå™¨å°è£…

## ä½¿ç”¨æ–¹æ³• (Usage)

### æ„å»ºé¡¹ç›®
```bash
cargo build
```

### è¿è¡Œ
```bash
cargo run
```

### è¿è¡Œæµ‹è¯•
```bash
cargo test
```

## å®ç°çŠ¶æ€ (Implementation Status)

### âœ… å·²å®Œæˆ
- [x] åŸºç¡€çŸ©é˜µå’Œå‘é‡è¿ç®—
- [x] æ¿€æ´»å‡½æ•°å®ç°
- [x] çº¿æ€§å±‚
- [x] å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
- [x] å±‚å½’ä¸€åŒ–
- [x] å‰é¦ˆç½‘ç»œ
- [x] Transformer ç¼–ç å™¨å’Œè§£ç å™¨å—
- [x] æŸå¤±å‡½æ•°ï¼ˆäº¤å‰ç†µã€MSEï¼‰
- [x] ä¼˜åŒ–å™¨ï¼ˆSGDã€ç®€åŒ–ç‰ˆ Adamï¼‰

### ğŸš§ å¾…å®Œæˆ
- [ ] Token åµŒå…¥å±‚ï¼ˆEmbeddingï¼‰
- [ ] ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰
- [ ] å®Œæ•´çš„åå‘ä¼ æ’­å®ç°
- [ ] æ¢¯åº¦ç´¯ç§¯å’Œæ›´æ–°
- [ ] æ•°æ®åŠ è½½å™¨
- [ ] è®­ç»ƒå¾ªç¯å®Œå–„
- [ ] æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
- [ ] è¯„ä¼°æŒ‡æ ‡ï¼ˆå‡†ç¡®ç‡ç­‰ï¼‰

## è®¾è®¡åŸåˆ™ (Design Principles)

1. **é›¶ä¾èµ–**: ä»…ä½¿ç”¨ Rust æ ‡å‡†åº“
2. **æ•™è‚²æ€§**: ä»£ç æ¸…æ™°ï¼Œæ³¨é‡Šè¯¦ç»†ï¼Œä¾¿äºç†è§£ Transformer åŸç†
3. **æ¨¡å—åŒ–**: æ¯ä¸ªç»„ä»¶ç‹¬ç«‹ï¼Œæ˜“äºæµ‹è¯•å’Œæ‰©å±•
4. **ç®€æ´æ€§**: ä¼˜å…ˆå®ç°æ ¸å¿ƒåŠŸèƒ½ï¼Œé¿å…è¿‡åº¦å·¥ç¨‹åŒ–

## å­¦ä¹ èµ„æº (Learning Resources)

è¿™ä¸ªé¡¹ç›®æ—¨åœ¨å¸®åŠ©ç†è§£ Transformer æ¶æ„çš„æ ¸å¿ƒåŸç†ï¼š
- Attention Is All You Need (Vaswani et al., 2017)
- The Illustrated Transformer (Jay Alammar)
- å„ç§ä»é›¶å®ç° Transformer çš„æ•™ç¨‹

## è®¸å¯è¯ (License)

MIT License

## è´¡çŒ® (Contributing)

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼
